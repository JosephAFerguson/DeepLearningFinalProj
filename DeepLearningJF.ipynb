{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephAFerguson/DeepLearningFinalProj/blob/main/DeepLearningJF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from requests import Session\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "YwvFxCVlJKZG"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. CoinMarketCap API Helper\n",
        "# ----------------------------\n",
        "class CryptoEndpoint:\n",
        "    listingsEndpoint = \"https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest\"\n",
        "    latestQuotes = \"https://pro-api.coinmarketcap.com/v2/cryptocurrency/quotes/latest\"\n",
        "    historicalQuotes = \"https://pro-api.coinmarketcap.com/v2/cryptocurrency/quotes/historical\"\n",
        "\n",
        "    def __init__(self, apikey) -> None:\n",
        "        self.headers = {\n",
        "            'Accepts': 'application/json',\n",
        "            'X-CMC_PRO_API_KEY': apikey,\n",
        "        }\n",
        "        self.coinsInfo = {}\n",
        "        self.coinsIds = []\n",
        "\n",
        "    def GetCoinIdentifiers(self, limit=10):\n",
        "        session = Session()\n",
        "        session.headers.update(self.headers)\n",
        "        response = session.get(\n",
        "            url=self.listingsEndpoint,\n",
        "            params={\"limit\": limit, \"price_min\": 1, \"price_max\": 20}\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Error fetching listings: {response.text}\")\n",
        "\n",
        "        data = json.loads(response.text)\n",
        "\n",
        "        for coin in data.get(\"data\", []):\n",
        "            self.coinsInfo[coin[\"name\"]] = coin[\"id\"]\n",
        "            self.coinsIds.append(coin[\"id\"])\n",
        "\n",
        "        print(f\"Loaded {len(self.coinsInfo)} coins.\")\n",
        "        return (self.coinsInfo, self.coinsIds)\n",
        "\n",
        "    def GetSampleCoinHistoricalData(self, coin_id, days=60):\n",
        "        \"\"\"Fetch historical daily prices for one coin.\"\"\"\n",
        "        session = Session()\n",
        "        session.headers.update(self.headers)\n",
        "\n",
        "        end_time = datetime.utcnow()\n",
        "        start_time = end_time - timedelta(days=days)\n",
        "\n",
        "        params = {\n",
        "            \"id\": coin_id,\n",
        "            \"time_start\": start_time.isoformat(),\n",
        "            \"time_end\": end_time.isoformat(),\n",
        "            \"interval\": \"24h\",\n",
        "        }\n",
        "\n",
        "        response = session.get(url=self.historicalQuotes, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error for coin {coin_id}: {response.text}\")\n",
        "            return []\n",
        "\n",
        "        data = json.loads(response.text)\n",
        "        if data.get(\"status\", {}).get(\"error_code\") != 0:\n",
        "            print(f\"API error for {coin_id}: {data['status']}\")\n",
        "            return []\n",
        "\n",
        "        coin_data = data.get(\"data\", {})\n",
        "        quotes = coin_data.get(\"quotes\", [])\n",
        "        if not quotes:\n",
        "            return []\n",
        "\n",
        "        historicals = []\n",
        "        for quote in quotes:\n",
        "            price = quote[\"quote\"][\"USD\"][\"price\"]\n",
        "            volume = quote[\"quote\"][\"USD\"][\"volume_24h\"]\n",
        "            marketcap = quote[\"quote\"][\"USD\"][\"market_cap\"]\n",
        "            circulating_supply = quote[\"quote\"][\"USD\"][\"circulating_supply\"]\n",
        "            total_supply = quote[\"quote\"][\"USD\"][\"total_supply\"]\n",
        "            historicals.append([price, volume, marketcap, circulating_supply, total_supply])\n",
        "\n",
        "        return historicals"
      ],
      "metadata": {
        "id": "nP90iGq4hMBO"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim,\n",
        "                 bidirectional=True, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=layer_dim,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if layer_dim > 1 else 0.0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Fully connected layer must match bidirectional hidden size:\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initial hidden states: [num_layers * num_directions, batch, hidden_dim]\n",
        "        h0 = torch.zeros(self.layer_dim * self.num_directions, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim * self.num_directions, batch_size, self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Take the last timestep\n",
        "        out = out[:, -1, :]  # shape: [batch, hidden_dim * num_directions]\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out   # logits for BCEWithLogitsLoss"
      ],
      "metadata": {
        "id": "EOb_EeFeiDck"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3. Data Preparation Utility\n",
        "# ----------------------------\n",
        "def prepare_sequences(data_dict, window=10):\n",
        "    \"\"\"\n",
        "    Convert dict {coin_id: [[price, volume], ...]} into windowed tensors\n",
        "    Target: 1 if next price > current, else 0\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    X, Y, labels = [], [], []\n",
        "\n",
        "    for coin_id, seq in data_dict.items():\n",
        "        arr = np.array(seq)\n",
        "        if len(arr) <= window + 1:\n",
        "            continue\n",
        "        scaled = scaler.fit_transform(arr)\n",
        "        prices = arr[:, 0]\n",
        "\n",
        "        for i in range(len(scaled) - window - 1):\n",
        "            X.append(scaled[i:i+window])\n",
        "            next_dir = 1.0 if prices[i+window+1] > prices[i+window] else 0.0\n",
        "            Y.append(next_dir)\n",
        "            labels.append(coin_id)\n",
        "\n",
        "    X = torch.tensor(np.array(X), dtype=torch.float32)\n",
        "    Y = torch.tensor(np.array(Y), dtype=torch.float32).view(-1, 1)\n",
        "    return X, Y, labels"
      ],
      "metadata": {
        "id": "HXLRjxsJEcJa"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainX, trainY, valX, valY, epochs=200, lr=0.01, patience=20):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.2)\n",
        "\n",
        "    # Ensure tensors\n",
        "    trainX = torch.tensor(trainX, dtype=torch.float32)\n",
        "    trainY = torch.tensor(trainY, dtype=torch.float32)\n",
        "    valX   = torch.tensor(valX,   dtype=torch.float32)\n",
        "    valY   = torch.tensor(valY,   dtype=torch.float32)\n",
        "\n",
        "    noImprovment = 0\n",
        "    best_comb_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----------------------------\n",
        "        # Training\n",
        "        # ----------------------------\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(trainX)\n",
        "        loss = criterion(outputs, trainY)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ----------------------------\n",
        "        # Validation\n",
        "        # ----------------------------\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(valX)\n",
        "            val_loss = criterion(val_outputs, valY)\n",
        "\n",
        "            pred_train = torch.sigmoid(outputs)\n",
        "            train_acc = (pred_train.round() == trainY).float().mean().item()\n",
        "\n",
        "            pred_val = torch.sigmoid(val_outputs)\n",
        "            val_acc = (pred_val.round() == valY).float().mean().item()\n",
        "\n",
        "        # Step scheduler using validation loss\n",
        "        scheduler.step(val_loss)\n",
        "        if (train_acc+val_acc)/2 > best_comb_acc:\n",
        "          noImprovment = 0\n",
        "          best_comb_acc = (train_acc+val_acc)/2\n",
        "        elif noImprovment>patience:\n",
        "          return model\n",
        "        else:\n",
        "          noImprovment+=1\n",
        "\n",
        "        # Status print every epoch\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "            f\"Train Loss: {loss.item():.5f} | Train Acc: {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss: {val_loss.item():.5f} | Val Acc: {val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "TF-YsBe6INRI"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. Get and Save Data as CSV\n",
        "# ----------------------------\n",
        "def GetSaveData():\n",
        "    apikey = input(\"Enter your CoinMarketCap API key: \").strip()\n",
        "    ce = CryptoEndpoint(apikey)\n",
        "\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # Optionally control how \"fresh\" the data should be before refetching\n",
        "    freshness_days = 1  # re-download if older than this many days\n",
        "\n",
        "    coinsInfo, coinsIds = ce.GetCoinIdentifiers(limit=10)\n",
        "    print(\"Fetching historical data (or loading cached files)...\")\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for coinId in coinsIds:\n",
        "        file_path = os.path.join(data_dir, f\"{coinId}.csv\")\n",
        "\n",
        "        # Check if we already have cached data\n",
        "        if os.path.exists(file_path):\n",
        "            modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
        "            if datetime.now() - modified_time < timedelta(days=freshness_days):\n",
        "                print(f\"[INFO] Using cached data for {coinId}\")\n",
        "                df = pd.read_csv(file_path)\n",
        "                all_data[coinId] = df\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"[INFO] Cached data for {coinId} is old. Refetching...\")\n",
        "\n",
        "        # Fetch new data from API\n",
        "        hist = ce.GetSampleCoinHistoricalData(coinId, days=60)\n",
        "        if len(hist) > 0:\n",
        "            df = pd.DataFrame(hist)\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"[INFO] Saved new data for {coinId} → {file_path}\")\n",
        "            all_data[coinId] = df\n",
        "        else:\n",
        "            print(f\"[WARN] No data found for {coinId}\")\n",
        "\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    print(f\"\\n✅ Fetched or loaded data for {len(all_data)} coins.\")\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "L9Qu8SF2LAyy"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6. Main Execution\n",
        "# ----------------------------\n",
        "all_data = GetSaveData()\n",
        "\n",
        "# Prepare sequences (X_all, Y_all, and matching labels)\n",
        "X_all, Y_all, labels = prepare_sequences(all_data, window=10)\n",
        "\n",
        "# Train / Val / Test split (60 / 20 / 20)\n",
        "trainX, restX, trainY, restY = train_test_split(\n",
        "    X_all, Y_all, test_size=0.4, shuffle=True\n",
        ")\n",
        "valX, testX, valY, testY = train_test_split(\n",
        "    restX, restY, test_size=0.5, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train Samples: {len(trainX)}\")\n",
        "print(f\"Val Samples:   {len(valX)}\")\n",
        "print(f\"Test Samples:  {len(testX)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Train model\n",
        "# ----------------------------\n",
        "model = LSTMModel(input_dim=5, hidden_dim=64, layer_dim=2, output_dim=1)\n",
        "trained_model = train_model(model, trainX, trainY, valX, valY, epochs=200, lr=0.01)\n",
        "\n",
        "# ----------------------------\n",
        "# Test Evaluation\n",
        "# ----------------------------\n",
        "trained_model.eval()\n",
        "\n",
        "testX_t = torch.tensor(testX, dtype=torch.float32)\n",
        "testY_t = torch.tensor(testY, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = trained_model(testX_t)\n",
        "    preds = torch.sigmoid(logits)\n",
        "    pred_classes = preds.round()\n",
        "\n",
        "    test_loss = nn.BCEWithLogitsLoss()(logits, testY_t).item()\n",
        "    test_acc = (pred_classes == testY_t).float().mean().item()\n",
        "\n",
        "print(\"\\n===== TEST RESULTS =====\")\n",
        "print(f\"Test Loss: {test_loss:.5f}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(\"========================\\n\")\n"
      ],
      "metadata": {
        "id": "18Jv9FnjzyU7",
        "outputId": "7d851b68-2ccc-4723-f2cc-3329e309b0dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your CoinMarketCap API key: a76bd6fc-2b66-4a25-843f-321def3437bd\n",
            "Loaded 10 coins.\n",
            "Fetching historical data (or loading cached files)...\n",
            "[INFO] Using cached data for 3408\n",
            "[INFO] Using cached data for 1975\n",
            "[INFO] Using cached data for 3957\n",
            "[INFO] Using cached data for 5805\n",
            "[INFO] Using cached data for 11419\n",
            "[INFO] Using cached data for 7083\n",
            "[INFO] Using cached data for 6636\n",
            "[INFO] Using cached data for 8916\n",
            "[INFO] Using cached data for 6535\n",
            "[INFO] Using cached data for 1321\n",
            "\n",
            "✅ Fetched or loaded data for 10 coins.\n",
            "Train Samples: 288\n",
            "Val Samples:   96\n",
            "Test Samples:  96\n",
            "Epoch [1/200] | Train Loss: 0.69241 | Train Acc: 53.47% | Val Loss: 0.70997 | Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1121255939.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trainX = torch.tensor(trainX, dtype=torch.float32)\n",
            "/tmp/ipython-input-1121255939.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trainY = torch.tensor(trainY, dtype=torch.float32)\n",
            "/tmp/ipython-input-1121255939.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  valX   = torch.tensor(valX,   dtype=torch.float32)\n",
            "/tmp/ipython-input-1121255939.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  valY   = torch.tensor(valY,   dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/200] | Train Loss: 0.69704 | Train Acc: 53.47% | Val Loss: 0.69586 | Val Acc: 50.00%\n",
            "Epoch [3/200] | Train Loss: 0.69030 | Train Acc: 53.47% | Val Loss: 0.69376 | Val Acc: 50.00%\n",
            "Epoch [4/200] | Train Loss: 0.69110 | Train Acc: 53.47% | Val Loss: 0.69365 | Val Acc: 50.00%\n",
            "Epoch [5/200] | Train Loss: 0.69123 | Train Acc: 53.47% | Val Loss: 0.69391 | Val Acc: 50.00%\n",
            "Epoch [6/200] | Train Loss: 0.69060 | Train Acc: 53.47% | Val Loss: 0.69463 | Val Acc: 50.00%\n",
            "Epoch [7/200] | Train Loss: 0.68981 | Train Acc: 53.47% | Val Loss: 0.69593 | Val Acc: 50.00%\n",
            "Epoch [8/200] | Train Loss: 0.68937 | Train Acc: 53.47% | Val Loss: 0.69756 | Val Acc: 50.00%\n",
            "Epoch [9/200] | Train Loss: 0.68945 | Train Acc: 53.47% | Val Loss: 0.69837 | Val Acc: 50.00%\n",
            "Epoch [10/200] | Train Loss: 0.68888 | Train Acc: 53.47% | Val Loss: 0.69797 | Val Acc: 50.00%\n",
            "Epoch [11/200] | Train Loss: 0.68743 | Train Acc: 53.47% | Val Loss: 0.69701 | Val Acc: 50.00%\n",
            "Epoch [12/200] | Train Loss: 0.68627 | Train Acc: 53.47% | Val Loss: 0.69665 | Val Acc: 46.88%\n",
            "Epoch [13/200] | Train Loss: 0.68467 | Train Acc: 57.29% | Val Loss: 0.69863 | Val Acc: 44.79%\n",
            "Epoch [14/200] | Train Loss: 0.68314 | Train Acc: 57.99% | Val Loss: 0.70325 | Val Acc: 45.83%\n",
            "Epoch [15/200] | Train Loss: 0.67736 | Train Acc: 57.29% | Val Loss: 0.71310 | Val Acc: 46.88%\n",
            "Epoch [16/200] | Train Loss: 0.67207 | Train Acc: 57.99% | Val Loss: 0.71409 | Val Acc: 46.88%\n",
            "Epoch [17/200] | Train Loss: 0.66988 | Train Acc: 58.33% | Val Loss: 0.71642 | Val Acc: 45.83%\n",
            "Epoch [18/200] | Train Loss: 0.66773 | Train Acc: 57.64% | Val Loss: 0.71957 | Val Acc: 48.96%\n",
            "Epoch [19/200] | Train Loss: 0.66539 | Train Acc: 60.07% | Val Loss: 0.72083 | Val Acc: 48.96%\n",
            "Epoch [20/200] | Train Loss: 0.66301 | Train Acc: 59.72% | Val Loss: 0.72084 | Val Acc: 45.83%\n",
            "Epoch [21/200] | Train Loss: 0.66064 | Train Acc: 58.33% | Val Loss: 0.72143 | Val Acc: 46.88%\n",
            "Epoch [22/200] | Train Loss: 0.65584 | Train Acc: 57.99% | Val Loss: 0.72257 | Val Acc: 46.88%\n",
            "Epoch [23/200] | Train Loss: 0.65521 | Train Acc: 58.68% | Val Loss: 0.72481 | Val Acc: 45.83%\n",
            "Epoch [24/200] | Train Loss: 0.65045 | Train Acc: 59.03% | Val Loss: 0.72675 | Val Acc: 43.75%\n",
            "Epoch [25/200] | Train Loss: 0.64462 | Train Acc: 60.42% | Val Loss: 0.72426 | Val Acc: 45.83%\n",
            "Epoch [26/200] | Train Loss: 0.64014 | Train Acc: 59.72% | Val Loss: 0.72018 | Val Acc: 46.88%\n",
            "Epoch [27/200] | Train Loss: 0.63640 | Train Acc: 60.07% | Val Loss: 0.72010 | Val Acc: 46.88%\n",
            "Epoch [28/200] | Train Loss: 0.63347 | Train Acc: 60.76% | Val Loss: 0.72070 | Val Acc: 48.96%\n",
            "Epoch [29/200] | Train Loss: 0.63354 | Train Acc: 60.76% | Val Loss: 0.72157 | Val Acc: 48.96%\n",
            "Epoch [30/200] | Train Loss: 0.63490 | Train Acc: 63.54% | Val Loss: 0.72245 | Val Acc: 50.00%\n",
            "Epoch [31/200] | Train Loss: 0.63137 | Train Acc: 61.46% | Val Loss: 0.72262 | Val Acc: 50.00%\n",
            "Epoch [32/200] | Train Loss: 0.62771 | Train Acc: 62.85% | Val Loss: 0.72218 | Val Acc: 50.00%\n",
            "Epoch [33/200] | Train Loss: 0.62890 | Train Acc: 63.54% | Val Loss: 0.72123 | Val Acc: 50.00%\n",
            "Epoch [34/200] | Train Loss: 0.63068 | Train Acc: 62.15% | Val Loss: 0.71967 | Val Acc: 48.96%\n",
            "Epoch [35/200] | Train Loss: 0.62542 | Train Acc: 65.28% | Val Loss: 0.71830 | Val Acc: 48.96%\n",
            "Epoch [36/200] | Train Loss: 0.62273 | Train Acc: 62.85% | Val Loss: 0.71753 | Val Acc: 47.92%\n",
            "Epoch [37/200] | Train Loss: 0.62315 | Train Acc: 63.89% | Val Loss: 0.71733 | Val Acc: 48.96%\n",
            "Epoch [38/200] | Train Loss: 0.62568 | Train Acc: 63.89% | Val Loss: 0.71746 | Val Acc: 50.00%\n",
            "Epoch [39/200] | Train Loss: 0.62007 | Train Acc: 64.93% | Val Loss: 0.71766 | Val Acc: 47.92%\n",
            "Epoch [40/200] | Train Loss: 0.62196 | Train Acc: 64.24% | Val Loss: 0.71792 | Val Acc: 47.92%\n",
            "Epoch [41/200] | Train Loss: 0.62489 | Train Acc: 62.50% | Val Loss: 0.71805 | Val Acc: 47.92%\n",
            "Epoch [42/200] | Train Loss: 0.62120 | Train Acc: 65.28% | Val Loss: 0.71813 | Val Acc: 47.92%\n",
            "Epoch [43/200] | Train Loss: 0.62069 | Train Acc: 65.28% | Val Loss: 0.71812 | Val Acc: 47.92%\n",
            "Epoch [44/200] | Train Loss: 0.62104 | Train Acc: 65.28% | Val Loss: 0.71815 | Val Acc: 47.92%\n",
            "Epoch [45/200] | Train Loss: 0.62043 | Train Acc: 63.19% | Val Loss: 0.71827 | Val Acc: 47.92%\n",
            "Epoch [46/200] | Train Loss: 0.62144 | Train Acc: 65.62% | Val Loss: 0.71837 | Val Acc: 47.92%\n",
            "Epoch [47/200] | Train Loss: 0.62212 | Train Acc: 62.15% | Val Loss: 0.71841 | Val Acc: 48.96%\n",
            "Epoch [48/200] | Train Loss: 0.61842 | Train Acc: 65.62% | Val Loss: 0.71842 | Val Acc: 48.96%\n",
            "Epoch [49/200] | Train Loss: 0.61566 | Train Acc: 65.28% | Val Loss: 0.71839 | Val Acc: 48.96%\n",
            "Epoch [50/200] | Train Loss: 0.61720 | Train Acc: 65.28% | Val Loss: 0.71836 | Val Acc: 48.96%\n",
            "Epoch [51/200] | Train Loss: 0.61697 | Train Acc: 64.24% | Val Loss: 0.71833 | Val Acc: 48.96%\n",
            "Epoch [52/200] | Train Loss: 0.61882 | Train Acc: 64.93% | Val Loss: 0.71829 | Val Acc: 48.96%\n",
            "Epoch [53/200] | Train Loss: 0.61715 | Train Acc: 65.97% | Val Loss: 0.71827 | Val Acc: 50.00%\n",
            "Epoch [54/200] | Train Loss: 0.62330 | Train Acc: 64.24% | Val Loss: 0.71822 | Val Acc: 50.00%\n",
            "Epoch [55/200] | Train Loss: 0.62081 | Train Acc: 63.54% | Val Loss: 0.71816 | Val Acc: 50.00%\n",
            "Epoch [56/200] | Train Loss: 0.61742 | Train Acc: 63.54% | Val Loss: 0.71809 | Val Acc: 50.00%\n",
            "Epoch [57/200] | Train Loss: 0.61546 | Train Acc: 66.32% | Val Loss: 0.71805 | Val Acc: 50.00%\n",
            "Epoch [58/200] | Train Loss: 0.61905 | Train Acc: 64.58% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [59/200] | Train Loss: 0.61378 | Train Acc: 65.62% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [60/200] | Train Loss: 0.61598 | Train Acc: 64.93% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [61/200] | Train Loss: 0.61572 | Train Acc: 64.58% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [62/200] | Train Loss: 0.61639 | Train Acc: 65.28% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [63/200] | Train Loss: 0.61686 | Train Acc: 66.32% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [64/200] | Train Loss: 0.61727 | Train Acc: 63.89% | Val Loss: 0.71802 | Val Acc: 50.00%\n",
            "Epoch [65/200] | Train Loss: 0.61673 | Train Acc: 64.24% | Val Loss: 0.71803 | Val Acc: 50.00%\n",
            "Epoch [66/200] | Train Loss: 0.61618 | Train Acc: 67.71% | Val Loss: 0.71804 | Val Acc: 50.00%\n",
            "Epoch [67/200] | Train Loss: 0.61992 | Train Acc: 64.58% | Val Loss: 0.71804 | Val Acc: 50.00%\n",
            "Epoch [68/200] | Train Loss: 0.61758 | Train Acc: 64.24% | Val Loss: 0.71805 | Val Acc: 50.00%\n",
            "Epoch [69/200] | Train Loss: 0.61910 | Train Acc: 64.24% | Val Loss: 0.71806 | Val Acc: 50.00%\n",
            "Epoch [70/200] | Train Loss: 0.61704 | Train Acc: 65.62% | Val Loss: 0.71807 | Val Acc: 50.00%\n",
            "Epoch [71/200] | Train Loss: 0.62089 | Train Acc: 63.54% | Val Loss: 0.71808 | Val Acc: 50.00%\n",
            "Epoch [72/200] | Train Loss: 0.62035 | Train Acc: 65.28% | Val Loss: 0.71808 | Val Acc: 50.00%\n",
            "Epoch [73/200] | Train Loss: 0.61528 | Train Acc: 64.24% | Val Loss: 0.71808 | Val Acc: 50.00%\n",
            "Epoch [74/200] | Train Loss: 0.61930 | Train Acc: 64.93% | Val Loss: 0.71808 | Val Acc: 50.00%\n",
            "Epoch [75/200] | Train Loss: 0.62018 | Train Acc: 63.19% | Val Loss: 0.71809 | Val Acc: 50.00%\n",
            "Epoch [76/200] | Train Loss: 0.61875 | Train Acc: 62.50% | Val Loss: 0.71809 | Val Acc: 50.00%\n",
            "Epoch [77/200] | Train Loss: 0.61804 | Train Acc: 65.28% | Val Loss: 0.71809 | Val Acc: 50.00%\n",
            "Epoch [78/200] | Train Loss: 0.61693 | Train Acc: 64.93% | Val Loss: 0.71809 | Val Acc: 50.00%\n",
            "Epoch [79/200] | Train Loss: 0.61586 | Train Acc: 66.32% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [80/200] | Train Loss: 0.61951 | Train Acc: 63.19% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [81/200] | Train Loss: 0.61977 | Train Acc: 62.85% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [82/200] | Train Loss: 0.61774 | Train Acc: 64.24% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [83/200] | Train Loss: 0.62112 | Train Acc: 65.28% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [84/200] | Train Loss: 0.61591 | Train Acc: 65.28% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [85/200] | Train Loss: 0.62034 | Train Acc: 64.58% | Val Loss: 0.71810 | Val Acc: 50.00%\n",
            "Epoch [86/200] | Train Loss: 0.61578 | Train Acc: 65.97% | Val Loss: 0.71811 | Val Acc: 50.00%\n",
            "Epoch [87/200] | Train Loss: 0.61725 | Train Acc: 64.58% | Val Loss: 0.71811 | Val Acc: 50.00%\n",
            "\n",
            "===== TEST RESULTS =====\n",
            "Test Loss: 0.73196\n",
            "Test Accuracy: 55.21%\n",
            "========================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2606506786.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testX_t = torch.tensor(testX, dtype=torch.float32)\n",
            "/tmp/ipython-input-2606506786.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  testY_t = torch.tensor(testY, dtype=torch.float32)\n"
          ]
        }
      ]
    }
  ]
}