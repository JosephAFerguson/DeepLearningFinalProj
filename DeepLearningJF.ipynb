{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephAFerguson/DeepLearningFinalProj/blob/main/DeepLearningJF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, os\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from requests import Session\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "YwvFxCVlJKZG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. CoinMarketCap API Helper\n",
        "# ----------------------------\n",
        "class CryptoEndpoint:\n",
        "    listingsEndpoint = \"https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest\"\n",
        "    latestQuotes = \"https://pro-api.coinmarketcap.com/v2/cryptocurrency/quotes/latest\"\n",
        "    historicalQuotes = \"https://pro-api.coinmarketcap.com/v2/cryptocurrency/quotes/historical\"\n",
        "\n",
        "    def __init__(self, apikey) -> None:\n",
        "        self.headers = {\n",
        "            'Accepts': 'application/json',\n",
        "            'X-CMC_PRO_API_KEY': apikey,\n",
        "        }\n",
        "        self.coinsInfo = {}\n",
        "        self.coinsIds = []\n",
        "\n",
        "    def GetCoinIdentifiers(self, limit=10):\n",
        "        session = Session()\n",
        "        session.headers.update(self.headers)\n",
        "        response = session.get(\n",
        "            url=self.listingsEndpoint,\n",
        "            params={\"limit\": limit, \"price_min\": 1, \"price_max\": 20}\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Error fetching listings: {response.text}\")\n",
        "\n",
        "        data = json.loads(response.text)\n",
        "\n",
        "        for coin in data.get(\"data\", []):\n",
        "            self.coinsInfo[coin[\"name\"]] = coin[\"id\"]\n",
        "            self.coinsIds.append(coin[\"id\"])\n",
        "\n",
        "        print(f\"Loaded {len(self.coinsInfo)} coins.\")\n",
        "        return (self.coinsInfo, self.coinsIds)\n",
        "\n",
        "    def GetSampleCoinHistoricalData(self, coin_id, days=60):\n",
        "        \"\"\"Fetch historical daily prices for one coin.\"\"\"\n",
        "        session = Session()\n",
        "        session.headers.update(self.headers)\n",
        "\n",
        "        end_time = datetime.utcnow()\n",
        "        start_time = end_time - timedelta(days=days)\n",
        "\n",
        "        params = {\n",
        "            \"id\": coin_id,\n",
        "            \"time_start\": start_time.isoformat(),\n",
        "            \"time_end\": end_time.isoformat(),\n",
        "            \"interval\": \"24h\",\n",
        "        }\n",
        "\n",
        "        response = session.get(url=self.historicalQuotes, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error for coin {coin_id}: {response.text}\")\n",
        "            return []\n",
        "\n",
        "        data = json.loads(response.text)\n",
        "        if data.get(\"status\", {}).get(\"error_code\") != 0:\n",
        "            print(f\"API error for {coin_id}: {data['status']}\")\n",
        "            return []\n",
        "\n",
        "        coin_data = data.get(\"data\", {})\n",
        "        quotes = coin_data.get(\"quotes\", [])\n",
        "        if not quotes:\n",
        "            return []\n",
        "\n",
        "        historicals = []\n",
        "        for quote in quotes:\n",
        "            price = quote[\"quote\"][\"USD\"][\"price\"]\n",
        "            volume = quote[\"quote\"][\"USD\"][\"volume_24h\"]\n",
        "            marketcap = quote[\"quote\"][\"USD\"][\"market_cap\"]\n",
        "            circulating_supply = quote[\"quote\"][\"USD\"][\"circulating_supply\"]\n",
        "            total_supply = quote[\"quote\"][\"USD\"][\"total_supply\"]\n",
        "            historicals.append([price, volume, marketcap, circulating_supply, total_supply])\n",
        "\n",
        "        return historicals"
      ],
      "metadata": {
        "id": "nP90iGq4hMBO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim,\n",
        "                 bidirectional=True, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.final_hidden = hidden_dim * self.num_directions\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=layer_dim,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if layer_dim > 1 else 0.0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # --- Attention layers ---\n",
        "        self.attn = nn.Linear(self.final_hidden, self.final_hidden)\n",
        "        self.v = nn.Linear(self.final_hidden, 1, bias=False)\n",
        "\n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(self.final_hidden, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initial hidden and cell states\n",
        "        h0 = torch.zeros(self.layer_dim * self.num_directions, batch_size, self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim * self.num_directions, batch_size, self.hidden_dim).to(x.device)\n",
        "\n",
        "        # LSTM outputs: (batch, seq_len, hidden_dim * num_directions)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # ---- Attention mechanism ----\n",
        "        # out → (batch, seq_len, hidden)\n",
        "        energy = torch.tanh(self.attn(out))                  # (batch, seq_len, hidden)\n",
        "        scores = self.v(energy).squeeze(-1)                  # (batch, seq_len)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=1)              # (batch, seq_len)\n",
        "\n",
        "        # Weighted sum: context vector\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), out)  # (batch, 1, hidden)\n",
        "        context = context.squeeze(1)                         # (batch, hidden)\n",
        "\n",
        "        # Final prediction\n",
        "        return self.fc(context), attn_weights\n"
      ],
      "metadata": {
        "id": "EOb_EeFeiDck"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3. Data Preparation Utility\n",
        "# ----------------------------\n",
        "def prepare_sequences(data_dict, window=10):\n",
        "    \"\"\"\n",
        "    Convert dict {coin_id: [[price, volume], ...]} into windowed tensors\n",
        "    Target: 1 if next price > current, else 0\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    X, Y, labels = [], [], []\n",
        "\n",
        "    for coin_id, seq in data_dict.items():\n",
        "        arr = np.array(seq)\n",
        "        if len(arr) <= window + 1:\n",
        "            continue\n",
        "        scaled = scaler.fit_transform(arr)\n",
        "        prices = arr[:, 0]\n",
        "\n",
        "        for i in range(len(scaled) - window - 1):\n",
        "            X.append(scaled[i:i+window])\n",
        "            next_dir = 1.0 if prices[i+window+1] > prices[i+window] else 0.0\n",
        "            Y.append(next_dir)\n",
        "            labels.append(coin_id)\n",
        "\n",
        "    X = torch.tensor(np.array(X), dtype=torch.float32)\n",
        "    Y = torch.tensor(np.array(Y), dtype=torch.float32).view(-1, 1)\n",
        "    return X, Y, labels"
      ],
      "metadata": {
        "id": "HXLRjxsJEcJa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, trainX, trainY, valX, valY, epochs=200, lr=0.01, patience=20):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.2)\n",
        "\n",
        "    # Ensure tensors\n",
        "    trainX = torch.tensor(trainX, dtype=torch.float32)\n",
        "    trainY = torch.tensor(trainY, dtype=torch.float32)\n",
        "    valX   = torch.tensor(valX,   dtype=torch.float32)\n",
        "    valY   = torch.tensor(valY,   dtype=torch.float32)\n",
        "\n",
        "    noImprovment = 0\n",
        "    best_comb_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----------------------------\n",
        "        # Training\n",
        "        # ----------------------------\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs,attn = model(trainX)\n",
        "        loss = criterion(outputs, trainY)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # ----------------------------\n",
        "        # Validation\n",
        "        # ----------------------------\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs,valattn = model(valX)\n",
        "            val_loss = criterion(val_outputs, valY)\n",
        "\n",
        "            pred_train = torch.sigmoid(outputs)\n",
        "            train_acc = (pred_train.round() == trainY).float().mean().item()\n",
        "\n",
        "            pred_val = torch.sigmoid(val_outputs)\n",
        "            val_acc = (pred_val.round() == valY).float().mean().item()\n",
        "\n",
        "        # Step scheduler using validation loss\n",
        "        scheduler.step(val_loss)\n",
        "        if (train_acc+val_acc)/2 > best_comb_acc:\n",
        "          noImprovment = 0\n",
        "          best_comb_acc = (train_acc+val_acc)/2\n",
        "        elif noImprovment>patience:\n",
        "          return model\n",
        "        else:\n",
        "          noImprovment+=1\n",
        "\n",
        "        # Status print every epoch\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "            f\"Train Loss: {loss.item():.5f} | Train Acc: {train_acc*100:.2f}% | \"\n",
        "            f\"Val Loss: {val_loss.item():.5f} | Val Acc: {val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "TF-YsBe6INRI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. Get and Save Data as CSV\n",
        "# ----------------------------\n",
        "def GetSaveData():\n",
        "    apikey = input(\"Enter your CoinMarketCap API key: \").strip()\n",
        "    ce = CryptoEndpoint(apikey)\n",
        "\n",
        "    data_dir = \"data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # Optionally control how \"fresh\" the data should be before refetching\n",
        "    freshness_days = 1  # re-download if older than this many days\n",
        "\n",
        "    coinsInfo, coinsIds = ce.GetCoinIdentifiers(limit=10)\n",
        "    print(\"Fetching historical data (or loading cached files)...\")\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for coinId in coinsIds:\n",
        "        file_path = os.path.join(data_dir, f\"{coinId}.csv\")\n",
        "\n",
        "        # Check if we already have cached data\n",
        "        if os.path.exists(file_path):\n",
        "            modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
        "            if datetime.now() - modified_time < timedelta(days=freshness_days):\n",
        "                print(f\"[INFO] Using cached data for {coinId}\")\n",
        "                df = pd.read_csv(file_path)\n",
        "                all_data[coinId] = df\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"[INFO] Cached data for {coinId} is old. Refetching...\")\n",
        "\n",
        "        # Fetch new data from API\n",
        "        hist = ce.GetSampleCoinHistoricalData(coinId, days=60)\n",
        "        if len(hist) > 0:\n",
        "            df = pd.DataFrame(hist)\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"[INFO] Saved new data for {coinId} → {file_path}\")\n",
        "            all_data[coinId] = df\n",
        "        else:\n",
        "            print(f\"[WARN] No data found for {coinId}\")\n",
        "\n",
        "        time.sleep(2)  # Avoid hitting API rate limits\n",
        "\n",
        "    print(f\"\\n✅ Fetched or loaded data for {len(all_data)} coins.\")\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "L9Qu8SF2LAyy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 6. Main Execution\n",
        "# ----------------------------\n",
        "all_data = GetSaveData()\n",
        "\n",
        "# Prepare sequences (X_all, Y_all, and matching labels)\n",
        "X_all, Y_all, labels = prepare_sequences(all_data, window=10)\n",
        "\n",
        "# Train / Val / Test split (60 / 20 / 20)\n",
        "trainX, restX, trainY, restY = train_test_split(\n",
        "    X_all, Y_all, test_size=0.4, shuffle=True\n",
        ")\n",
        "valX, testX, valY, testY = train_test_split(\n",
        "    restX, restY, test_size=0.5, shuffle=True\n",
        ")\n",
        "\n",
        "trainX_t = torch.tensor(trainX, dtype=torch.float32).to(device)\n",
        "trainY_t = torch.tensor(trainY, dtype=torch.float32).to(device)\n",
        "valX_t   = torch.tensor(valX, dtype=torch.float32).to(device)\n",
        "valY_t   = torch.tensor(valY, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "print(f\"Train Samples: {len(trainX)}\")\n",
        "print(f\"Val Samples:   {len(valX)}\")\n",
        "print(f\"Test Samples:  {len(testX)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Train model\n",
        "# ----------------------------\n",
        "model = LSTMModel(input_dim=5, hidden_dim=64, layer_dim=10, output_dim=1).to(device)\n",
        "trained_model = train_model(model, trainX, trainY, valX, valY, epochs=200, lr=0.01)\n",
        "\n",
        "# ----------------------------\n",
        "# Test Evaluation\n",
        "# ----------------------------\n",
        "trained_model.eval()\n",
        "\n",
        "testX_t = torch.tensor(testX, dtype=torch.float32).to(device)\n",
        "testY_t = torch.tensor(testY, dtype=torch.float32).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits,attn = trained_model(testX_t)\n",
        "    preds = torch.sigmoid(logits)\n",
        "    pred_classes = preds.round()\n",
        "\n",
        "    test_loss = nn.BCEWithLogitsLoss()(logits, testY_t).item()\n",
        "    test_acc = (pred_classes == testY_t).float().mean().item()\n",
        "\n",
        "print(\"\\n===== TEST RESULTS =====\")\n",
        "print(f\"Test Loss: {test_loss:.5f}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(\"========================\\n\")\n"
      ],
      "metadata": {
        "id": "18Jv9FnjzyU7",
        "outputId": "b155e91b-cdbc-4bf7-be79-c987719ddb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your CoinMarketCap API key: a76bd6fc-2b66-4a25-843f-321def3437bd\n",
            "Loaded 10 coins.\n",
            "Fetching historical data (or loading cached files)...\n",
            "[INFO] Using cached data for 3408\n",
            "[INFO] Using cached data for 1975\n",
            "[INFO] Using cached data for 3957\n",
            "[INFO] Using cached data for 5805\n",
            "[INFO] Using cached data for 7083\n",
            "[INFO] Using cached data for 6636\n",
            "[INFO] Using cached data for 11419\n",
            "[INFO] Using cached data for 6535\n",
            "[INFO] Using cached data for 8916\n",
            "[INFO] Using cached data for 1321\n",
            "\n",
            "✅ Fetched or loaded data for 10 coins.\n",
            "Train Samples: 288\n",
            "Val Samples:   96\n",
            "Test Samples:  96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2952578547.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trainX = torch.tensor(trainX, dtype=torch.float32)\n",
            "/tmp/ipython-input-2952578547.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trainY = torch.tensor(trainY, dtype=torch.float32)\n",
            "/tmp/ipython-input-2952578547.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  valX   = torch.tensor(valX,   dtype=torch.float32)\n",
            "/tmp/ipython-input-2952578547.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  valY   = torch.tensor(valY,   dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1776038046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2952578547.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, trainX, trainY, valX, valY, epochs, lr, patience)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2523255248.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# LSTM outputs: (batch, seq_len, hidden_dim * num_directions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# ---- Attention mechanism ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             result = _VF.lstm(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
          ]
        }
      ]
    }
  ]
}